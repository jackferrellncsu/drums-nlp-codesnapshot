{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2b11d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "import pickle\n",
    "import os\n",
    "import json\n",
    "import demonstration_routine as helper\n",
    "from torch.nn import Softmax \n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "import numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0ab18414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "#Initialize pre trained components of BERT model\n",
    "model =  BertForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "softmax = Softmax(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3779fe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load calibration set of non-conformity scores\n",
    "def load_cal_alphas(filename):\n",
    "    path = \"Data/\" + filename + '.txt'\n",
    "    with open(path, \"r\") as file:\n",
    "        loaded_vals = json.load(file)\n",
    "    return loaded_vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "684b61bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gets index of masked word from tokenized tensor.\n",
    "def find_mask_ind(tokenized_sentence):\n",
    "    return tokenized_sentence.input_ids[0].tolist().index(103)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6c690f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Input clean sentences.  \n",
    "#Words should already be masked.\n",
    "def conf_pred(sentence, model, conf, calib, m_ind = -1):\n",
    "    q_soft = numpy.quantile(calib, conf)\n",
    "\n",
    "    conf_intervals = []\n",
    "\n",
    "    \n",
    "    input = tokenizer(sentence, return_tensors='pt')\n",
    "\n",
    "    #Get index of masked word\n",
    "    if m_ind == -1:\n",
    "        m_ind = find_mask_ind(input)\n",
    "    \n",
    "    outputs = model(**input)\n",
    "\n",
    "    result_softmax = softmax(outputs.logits[0][m_ind]).tolist()\n",
    "    result_softmax = numpy.array(result_softmax)\n",
    "\n",
    "    non_confs = 1 - result_softmax\n",
    "\n",
    "    region_inds = numpy.nonzero(non_confs <= q_soft)[0].tolist()\n",
    "\n",
    "    words = tokenizer.convert_ids_to_tokens(region_inds)\n",
    "\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5f1882f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load calibration set of non-conformity scores\n",
    "alphas = helper.load_cal_alphas(\"soft_alphas_0_0\")\n",
    "\n",
    "#Initialize test sentences from Michelle Obama ted talk\n",
    "test_sent_1 = \"...to go with him to a community [MASK]. But when we met, Barack was a community organizer.\" \n",
    "test_sent_2 = \"And he urged the people in that meeting in that community to devote themselves to closing the gap between those two ideas, to work together to try to make the world as it is and the world as it should [MASK]one and the same.\"\n",
    "test_sent_3 = \"And they opened many new doors for millions of female doctors and nurses and artists and authors all of whom have [MASK] [MASK]. And by getting a good education you too can control your own destiny.\"\n",
    "\n",
    "test4 = \"We want [MASK] for dinner\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c41c5418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['that',\n",
       " 'it',\n",
       " 'her',\n",
       " 'you',\n",
       " 'this',\n",
       " 'one',\n",
       " 'him',\n",
       " 'them',\n",
       " 'some',\n",
       " 'something',\n",
       " 'coffee']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confidence = 0.8\n",
    "conf_pred(test4, model, confidence, alphas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fc1798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
